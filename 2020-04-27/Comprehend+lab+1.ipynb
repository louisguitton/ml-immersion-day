{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pprint  #print json \n",
    "\n",
    "# Make some tests modifying “text_to_analyze” variable at the top of the script. For\n",
    "# more information about Comprehend capabilities please visit\n",
    "# https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html\n",
    "    \n",
    "text_to_analyze = \"The AWS platform was launched in July 2002, [5] in the beginning, the platform consisted of only a few disparate tools and services. Then in late 2003, the AWS concept was publicly reformulated when Chris Pinkham and Benjamin Black presented a paper describing a vision for Amazon's retail computing infrastructure that was completely standardized, completely automated, and would rely extensively on web services for services such as storage and would draw on internal work already underway. Near the end of their paper, they mentioned the possibility of selling access to virtual servers as a service, proposing the company could generate revenue from the new infrastructure investment.[11] In November 2004, the first AWS service launched for public usage: Simple Queue Service (SQS).[12] Thereafter Pinkham and lead developer Christoper Brown developed the Amazon EC2 service, with a team in Cape Town, South Africa.[13]\"\n",
    "\n",
    "comprehend = boto3.client(service_name='comprehend')\n",
    "\n",
    "        \n",
    "sentiment = comprehend.detect_sentiment(Text=text_to_analyze, LanguageCode='en')\n",
    "print(\"\\n\\nSentiment detected: \" + sentiment['Sentiment'])\n",
    "print(\"Sentiment detected (details): \")\n",
    "pprint.pprint(sentiment)\n",
    "\n",
    "print(\"\\n\\n\\nEntities detected: \\n\")\n",
    "entities = comprehend.detect_entities(Text=text_to_analyze, LanguageCode='en')\n",
    "if len(entities['Entities']) > 0:\n",
    "    for node in entities['Entities']:\n",
    "        print(node['Type'] + ': ' + node['Text'])\n",
    "\n",
    "print(\"\\n\\nEntities detected (details): \\n\")\n",
    "        \n",
    "pprint.pprint(entities)\n",
    "\n",
    "print(\"\\n\\nkey phrases founded: \\n\")\n",
    "key_phrases = comprehend.detect_key_phrases(Text=text_to_analyze, LanguageCode='en')\n",
    "\n",
    "if len(key_phrases['KeyPhrases']) > 0:\n",
    "    for node in key_phrases['KeyPhrases']:\n",
    "        print(node['Text'])\n",
    "print(\"\\n\\nkey phrases founded (details): \")\n",
    "pprint.pprint(key_phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
